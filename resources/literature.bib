@article{Blei2012a,
abstract = {We describe latent Dirichlet allocation (LDA), a generative probabilistic model for collections of discrete data such as text corpora. LDA is a three-level hierarchical Bayesian model, in which each item of a collection is modeled as a finite mixture over an underlying set of topics. Each topic is, in turn, modeled as an infinite mixture over an underlying set of topic probabilities. In the context of text modeling, the topic probabilities provide an explicit representation of a document. We present efficient approximate inference techniques based on variational methods and an EM algorithm for empirical Bayes parameter estimation. We report results in document modeling, text classification, and collaborative filtering, comparing to a mixture of unigrams model and the probabilistic LSI model.},
archivePrefix = {arXiv},
arxivId = {1111.6189v1},
author = {Blei, David M and Ng, Andrew Y and Jordan, Michael I},
doi = {10.1162/jmlr.2003.3.4-5.993},
eprint = {1111.6189v1},
file = {:Users/lukas/Downloads/blei03a.pdf:pdf},
isbn = {9781577352815},
issn = {15324435},
journal = {Journal of Machine Learning Research},
keywords = {lda,topic model},
pages = {993--1022},
pmid = {21362469},
title = {{Latent Dirichlet Allocation}},
url = {http://www.cs.princeton.edu/~blei/lda-c/$\backslash$npapers2://publication/doi/10.1162/jmlr.2003.3.4-5.993$\backslash$npapers2://publication/uuid/4001D0D9-4F9C-4D8F-AE49-46ED6A224F4A$\backslash$npapers2://publication/uuid/7D10D5DA-B421-4D94-A3ED-028107B7F9B6$\backslash$nhttp://www.crossref.org/jmlr},
volume = {3},
year = {2012}
}
@article{Blei2012,
author = {Blei, David M.},
doi = {10.1145/2133806.2133826},
file = {:Users/lukas/Downloads/p77-blei.pdf:pdf},
issn = {00010782},
journal = {Communications of the ACM},
month = apr,
number = {4},
pages = {77},
publisher = {ACM},
title = {{Probabilistic topic models}},
url = {http://dl.acm.org/ft\_gateway.cfm?id=2133826\&type=html},
volume = {55},
year = {2012}
}
@article{Grun2011,
abstract = {Topic models allow the probabilistic modeling of term frequency occurrences in doc- uments. The tted model can be used to estimate the similarity between documents as well as between a set of speci ed keywords using an additional layer of latent variables which are referred to as topics. The R package topicmodels provides basic infrastructure for tting topic models based on data structures from the text mining package tm. The package includes interfaces to two algorithms for tting topic models: the variational expectation-maximization algorithm provided by David M. Blei and co-authors and an algorithm using Gibbs sampling by Xuan-Hieu Phan and co-authors.},
author = {Gr\"{u}n, Bettina and Hornik, Kurt},
file = {:Users/lukas/Library/Application Support/Mendeley Desktop/Downloaded/Gr\"{u}n, Hornik - 2011 - topicmodels An R Package for Fitting Topic Models.pdf:pdf},
journal = {Journal of Statistical Software},
keywords = {gibbs sampling,r,text analysis,topic model,variational em},
number = {13},
pages = {1--30},
publisher = {American Statistical Association},
title = {{topicmodels : An R Package for Fitting Topic Models}},
url = {http://kortan.sote.hu/ftp/mirrors/CRAN/web/packages/topicmodels/vignettes/topicmodels.pdf},
volume = {40},
year = {2011}
}
@inproceedings{Hofmann1999,
abstract = {Probabilistic Latent Semantic Indexing is a novel approach to automated document indexing which is based on a statistical latent class model for factor analysis of count data. Fitted from a training corpus of text documents by a generalization of the Expectation Maximization algorithm, the utilized model is able to deal with domain-specific synonymy as well as with polysemous words. In con trast to standard Latent Semantic Indexing (LSI) by Singular Value Decomposition, the probabilistic variant has a solid statistical foundation and defines a proper generative data model. Retrieval experiments on a number of test collections indicate substantial performance gains over direct term matching methods as well as over LSI. In particular, the combination of models with different dimensionalities has proven to be advantageous.},
author = {Hofmann, Thomas},
booktitle = {SIGIR},
doi = {10.1145/312624.312649},
isbn = {1581130961},
issn = {15206882},
pages = {50--57},
title = {{Probabilistic latent semantic indexing}},
url = {http://portal.acm.org/citation.cfm?doid=312624.312649},
year = {1999}
}
@misc{Rolf,
author = {Rolf, Fredheim},
booktitle = {University of Cambridge},
title = {{Quantifying Memory: Web-Scraping: the Basics}},
url = {http://quantifyingmemory.blogspot.co.uk/2014/02/web-scraping-basics.html},
urldate = {2015-02-25}
}
@article{Sievert2014,
author = {Sievert, C and Shirley, KE},
file = {:Users/lukas/Library/Application Support/Mendeley Desktop/Downloaded/Sievert, Shirley - 2014 - LDAvis A method for visualizing and interpreting topics.pdf:pdf},
journal = {Sponsor: Idibon},
title = {{LDAvis: A method for visualizing and interpreting topics}},
url = {http://www.aclweb.org/anthology/W/W14/W14-31.pdf\#page=73},
year = {2014}
}
